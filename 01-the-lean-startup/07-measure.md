# 7. Measure
The financials in the business plan include projections of how many customers the company expects to attract, how much it will spend, and how much revenue and profit that will lead to. It's an ideal that's usually far from where the startup is in its early days.

A startup's job is to (1) rigorously measure where it is right now, confronting the hard truths that assessment reveals, and then (2) devise experiments to learn how to move the real numbers closer to the ideal reflected in the business plan.

Most products have some customers, some growth, and some positive results. One of the most dangerous outcomes for a startup is to bumble along in the land of the living dead.

We all know stories of epic entrepreneurs who managed to pull out a victory when things seemed incredibly bleak. Unfortunately, we don't hear stories about the countless nameless others who persevered too long, leading their companies to failure.

Accounting allowed GM to set clear milestones for each of its divisions and then hold each manager accountable for his or her division's success in reaching those goals. All modern corporations use some variation of that approach. Accounting is the key to their success.

Unfortunately, standard accounting is not helpful in evaluating entrepreneurs. Startups are too unpredictable for forecasts and milestones to be accurate.

How do we know that the changes we've made are related to the results we're seeing? More important, how do we know that we are drawing the right lessons from those changes?

Innovation accounting enables startups to prove objectively that they are learning how to grow a sustainable business.

Every business plan has some kind of model associated with it. That model provides assumptions about what the business will look like at a successful point in the future.

For example, the business plan for an established manufacturing company would show it growing in proportion to its sales volume. As the profits from the sales of goods are reinvested in marketing and promotions, the company gains new customers. The rate of growth depends primarily on three things: the profitability of each customer, the cost of acquiring new customers, and the repeat purchase rate of existing customers. The higher these values are, the faster the company will grow and the more profitable it will be. These are the drivers of the company's growth model.

eBay will have a different growth model. Its success depends primarily on the network effects that make it the premier destination for both buyers and sellers to transact business. Sellers want the marketplace with the highest number of potential customers. Buyers want the marketplace with the most competition among sellers, which leads to the greatest availability of products and the lowest prices.

For this kind of startup, the important thing to measure is that the network effects are working, as evidenced by the high retention rate of new buyers and sellers.

Innovation accounting works in three steps: first, use a minimum viable product to establish real data on where the company is right now.

Second, startups must attempt to tune the engine from the baseline toward the ideal.

The third step: pivot or persevere.

The sign of a successful pivot is that these engine-tuning activities are more productive after the pivot than before.

A smoke test: customers are given the opportunity to preorder a product that has not yet been built. It measures whether customers are interested in trying a product. By itself, this is insufficient to validate an entire growth model. Nonetheless, it can be very useful to get feedback on this assumption before committing more money and other resources to the product.

Conversion rates, sign-up and trial rates, customer lifetime value, and so on - and this is valuable as the foundation for learning about customers and their reactions to a product. 

It makes sense to test the riskiest assumptions first. If you can't find a way to mitigate these risks toward the ideal that is required for a sustainable business, there is no point in testing the others.

A media business that is selling advertising has two basic assumptions that take the form of questions: Can it capture the attention of a defined customer segment on an ongoing basis? and can it sell that attention to advertisers? The far riskier assumption is the ability to capture attention. Therefore, the first experiments should involve content production rather than advertising sales. Perhaps the company will produce a pilot episode or issue to see how customers engage.

Every product development, marketing, or other initiative that a startup undertakes should be targeted at improving one of the drivers of its growth model.

For example, a company might spend time improving the design of its product to make it easier for new customers to use. This presupposes that the activation rate of new customers is a driver of growth and that its baseline is lower than the company would like.

To demonstrate validated learning, the design changes must improve the activation rate of new customers. If they do not, the new design should be judged a failure.

The first company sets out with a clear baseline metric, a hypothesis about what will improve that metric, and a set of experiments designed to test that hypothesis. The second team sits around debating what would improve the product, implements several of those changes at once, and celebrates if there is any positive increase in any of the numbers. Which startup is more likely to be doing effective work?

If we're not moving the drivers of our business model, we're not making progress. That becomes a sure sign that it's time to pivot.

Five dollars bought us a hundred clicks - every day. From a marketing point of view this was not very significant, but for learning it was priceless. Every single day we were able to measure our product's performance with a brand new set of customers. Also, each time we revised the product, we got a brand new report card on how we were doing the very next day.

Instead of looking at cumulative totals or gross numbers such as total revenue and total number of customers, one looks at the performance of each group of customers that comes into contact with the product independently. Each group is called a cohort.

Every company depends for its survival on sequences of customer behavior called flows. Customer flows govern the interaction of customers with a company's products.

Once our efforts were aligned with what customers really wanted, our experiments were much more likely to change their behavior for the better.

The sign of a successful pivot: the new experiments you run are overall more productive than the experiments you were running before.

If you are building the wrong thing, optimizing the product or its marketing will not yield significant results.

A startup has to measure progress against a high bar: evidence that a sustainable business can be built around its products or services.

The traditional gross metrics for IMVU so far: total registered users and total paying customers (the gross revenue graph looks almost the same). From this viewpoint, things look much more exciting. That's why I call these vanity metrics: they give the rosiest possible picture. You'll see a traditional hockey stick graph (the ideal in a rapid-growth company). As long as you focus on the top-line numbers (signing up more customers, an increase in overall revenue), you'll be forgiven for thinking this product development team is making great progress.

But think back to the same data presented in a cohort style. IMVU is adding new customers, but it is not improving the yield on each new group.

From the traditional graph alone, you cannot tell whether IMVU is on pace to build a sustainable business; you certainly can't tell anything about the efficacy of the entrepreneurial team behind it.

I put this question to Farb: "How confident are you that you are making the right decisions in terms of establishing priorities?" Like most startup founders, he was looking at the available data and making the best educated guesses he could. But this left a lot of room for ambiguity and doubt.

Agile development: engineers agree to adapt the product to the business's constantly changing requirements, but are not responsible for the quality of those business decisions.

In every cycle, the type of metrics his team was focused on would change: one month they would look at gross usage numbers, another month registration numbers, and so on. Those metrics would go up and down seemingly on their own. He couldn't draw clear cause-and-effect inferences.

A split-test experiment is one in which different versions of a product are offered to customers at the same time. By observing the changes in behavior between the two groups, one can make inferences about the impact of the different variations.

If you wanted to test a catalog design, you could send a new version of it to 50% of the customers and send the old standard catalog to the other 50%.

To assure a scientific result, both catalogs would contain identical products; the only difference would be the changes to the design.

This technique is sometimes called A/B testing after the practice of assigning letter names to each variation.

Split testing often uncovers surprising things. For example, many features that make the product better in the eyes of engineers and designers have no impact on customer behavior.

Although working with split tests seems to be more difficult because it requires extra accounting and metrics to keep track of each variation, it almost always saves tremendous amounts of time in the long run by eliminating work that doesn't matter to customers.

Stories could be cataloged as being in 1 of 4 states of development: in the product backlog, actively being built, done (feature complete from a technical point of view), or in the process of being validated. Validated was defined as "knowing whether the story was a good idea to have been done in the first place".

This validation usually would come in the form of a split test showing a change in customer behavior, but also might include customer interviews or surveys.

The kanban rule permitted only so many stories in each of the four states. As stories flow from one state to the other, the buckets fill up. Once a bucket becomes full, it cannot accept more stories. Only when a story has been validated can it be removed from the kanban board. If the validation fails and it turns out the story is a bad idea, the relevant feature is removed from the product. 

No bucket can contain more than three projects at a time.

Why build a new feature that is not part of a split-test experiment? It may save you time in the short run, but it will take more time later to test, during the validation phase.

A solid process lays the foundation for a healthy culture, one where ideas are evaluated by merit and not by job title. 

Most important, teams working in this system begin to measure their productivity according to validated learning, not in terms of the production of new features.

They tested a feature called lazy registration, to see if it was worth the heavy investment they were making in ongoing support. In this system, customers do not have to register for the service up front. Instead, they immediately begin using the service and are asked to register only after they have had a chance to experience the service's benefit.

To their surprise, this cohort's behavior was exactly the same as that of the lazy registration group: they had the same rate of registration, activation, and subsequent retention. In other words, the extra effort of lazy registration was a complete waste even though it was considered an industry best practice.

The insight that this test suggested: customers were basing their decision on something other than their use of the product. This suggested that improving Grockit's positioning and marketing might have a more significant impact on attracting new customers than would adding new features.

3 A's of metrics: actionable, accessible, and auditable.

For a report to be considered actionable, it must demonstrate clear cause and effect.

When the numbers go up, people think the improvement was caused by their actions, by whatever they were working on at the time.

When cause and effect is clearly understood, people are better able to learn from their actions.

Unfortunately, most managers do not respond to this complexity by working hand in hand with the data warehousing team to simplify the reports so that they can understand them better.

Departments too often spend their energy learning how to use data to get what they want, rather than as genuine feedback to guide their future actions.

First, make the reports as simple as possible so that everyone understands them. Use tangible, concrete units.

Every day their system automatically generated a document containing the latest data for every single one of their split-test experiments and other leap-of-faith metrics. This document was mailed to every employee of the company: they all always had a fresh copy in their e-mail in-boxes. The reports were well laid out and easy to read, with each experiment and its results explained in plain English.

Each employee could log in to the system at any time, choose from a list of all current and past experiments, and see a simple one-page summary of the results. Over time, those one-page summaries became the de facto standard for settling product arguments throughout the organization.

We must ensure that the data is credible to employees.

Most of the time, when a manager, developer, or team was confronted with results that would kill a pet project, the loser of the argument would challenge the veracity of the data.

The solution? First, remember that "Metrics are people, too." We need to be able to test the data by hand, in the messy real world, by talking to customers.

Managers need the ability to spot check the data with real customers.

Systems that provide this level of auditability give managers and entrepreneurs the opportunity to gain insights into why customers are behaving the way the data indicate.

Only 5% of entrepreneurship is the big idea, the business model, the whiteboard strategizing, and the splitting up of the spoils. The other 95% is the gritty work that is measured by innovation accounting: product prioritization decisions, deciding which customers to target or listen to, and having the courage to subject a grand vision to constant testing and feedback.
